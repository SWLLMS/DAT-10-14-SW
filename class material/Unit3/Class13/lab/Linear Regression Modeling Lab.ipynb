{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Modeling Lab\n",
    "\n",
    "This lab will walk you through the basics of building a linear regression model out of a training and test set using a variety of techniques, including:\n",
    "\n",
    " - estimating distributional fit\n",
    " - onehot and target encoding\n",
    " - measuring progress with cross validation scores\n",
    " - creating a custom loss function\n",
    " - properly using inferences from the training set to transform the test set\n",
    " \n",
    "**Some of these columns might have missing values.  Decide on the best approach for filling them in based on what we did from last class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1).  Upload the training and test set from the `\\movies` folder inside the `\\Data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import probplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/swllms/DAT-10-14-SW/class material/Unit3/Data/movies/train 2.csv', parse_dates=['release_date'])\n",
    "test  = pd.read_csv('/Users/swllms/DAT-10-14-SW/class material/Unit3/Data/movies/test.csv', parse_dates=['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>budget</th>\n",
       "      <th>original_title</th>\n",
       "      <th>cast</th>\n",
       "      <th>homepage</th>\n",
       "      <th>director</th>\n",
       "      <th>tagline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>overview</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>release_year</th>\n",
       "      <th>budget_adj</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421043</td>\n",
       "      <td>0</td>\n",
       "      <td>The Unforgiven</td>\n",
       "      <td>Burt Lancaster|Audrey Hepburn|Audie Murphy|Joh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Huston</td>\n",
       "      <td>A NEW TRIUMPH FROM ACADEMY AWARD WINNER JOHN H...</td>\n",
       "      <td>indian|texas|farm|siblings|saddle</td>\n",
       "      <td>The neighbors of a frontier family turn on the...</td>\n",
       "      <td>125</td>\n",
       "      <td>Action|Drama|Western</td>\n",
       "      <td>James Productions</td>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>17</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333643</td>\n",
       "      <td>0</td>\n",
       "      <td>The Brides of Dracula</td>\n",
       "      <td>Peter Cushing|Martita Hunt|Yvonne Monlaur|Fred...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terence Fisher</td>\n",
       "      <td>He Turned Innocent Beauty Into Unspeakable Hor...</td>\n",
       "      <td>dracula|hammer horror|van helsing</td>\n",
       "      <td>A young teacher on her way to a position in Tr...</td>\n",
       "      <td>85</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Hammer Film Productions|Hotspur Film Productio...</td>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.194948</td>\n",
       "      <td>0</td>\n",
       "      <td>Sink the Bismarck!</td>\n",
       "      <td>Kenneth More|Dana Wynter|Carl MÃ¶hner|Laurence...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lewis Gilbert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A true WW2 story: the British Navy must find a...</td>\n",
       "      <td>97</td>\n",
       "      <td>Action|Drama|Foreign|History|War</td>\n",
       "      <td>Twentieth Century Fox Film Corporation</td>\n",
       "      <td>1960-02-11</td>\n",
       "      <td>12</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136525</td>\n",
       "      <td>0</td>\n",
       "      <td>Carry On Constable</td>\n",
       "      <td>Kenneth Connor|Charles Hawtrey|Sid James|Kenne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gerald Thomas</td>\n",
       "      <td>Oh! What a Carry On When that Crazy Bunch Join...</td>\n",
       "      <td>carry on|police station</td>\n",
       "      <td>With a flu epidemic running rife, three new bu...</td>\n",
       "      <td>86</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Peter Rogers Productions</td>\n",
       "      <td>1960-02-22</td>\n",
       "      <td>10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267118</td>\n",
       "      <td>0</td>\n",
       "      <td>Comanche Station</td>\n",
       "      <td>Randolph Scott|Nancy Gates|Skip Homeier|Dyke J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Budd Boetticher</td>\n",
       "      <td>The One-Man War Against The Comancheros!</td>\n",
       "      <td>indian|comanche</td>\n",
       "      <td>A man saves a woman who had been kidnapped by ...</td>\n",
       "      <td>73</td>\n",
       "      <td>Action|Western</td>\n",
       "      <td>Columbia Pictures Corporation|Ranown Pictures ...</td>\n",
       "      <td>1960-03-01</td>\n",
       "      <td>12</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  budget         original_title  \\\n",
       "0    0.421043       0         The Unforgiven   \n",
       "1    0.333643       0  The Brides of Dracula   \n",
       "2    0.194948       0     Sink the Bismarck!   \n",
       "3    0.136525       0     Carry On Constable   \n",
       "4    0.267118       0       Comanche Station   \n",
       "\n",
       "                                                cast homepage  \\\n",
       "0  Burt Lancaster|Audrey Hepburn|Audie Murphy|Joh...      NaN   \n",
       "1  Peter Cushing|Martita Hunt|Yvonne Monlaur|Fred...      NaN   \n",
       "2  Kenneth More|Dana Wynter|Carl MÃ¶hner|Laurence...      NaN   \n",
       "3  Kenneth Connor|Charles Hawtrey|Sid James|Kenne...      NaN   \n",
       "4  Randolph Scott|Nancy Gates|Skip Homeier|Dyke J...      NaN   \n",
       "\n",
       "          director                                            tagline  \\\n",
       "0      John Huston  A NEW TRIUMPH FROM ACADEMY AWARD WINNER JOHN H...   \n",
       "1   Terence Fisher  He Turned Innocent Beauty Into Unspeakable Hor...   \n",
       "2    Lewis Gilbert                                                NaN   \n",
       "3    Gerald Thomas  Oh! What a Carry On When that Crazy Bunch Join...   \n",
       "4  Budd Boetticher           The One-Man War Against The Comancheros!   \n",
       "\n",
       "                            keywords  \\\n",
       "0  indian|texas|farm|siblings|saddle   \n",
       "1  dracula|hammer horror|van helsing   \n",
       "2                                NaN   \n",
       "3            carry on|police station   \n",
       "4                    indian|comanche   \n",
       "\n",
       "                                            overview  runtime  \\\n",
       "0  The neighbors of a frontier family turn on the...      125   \n",
       "1  A young teacher on her way to a position in Tr...       85   \n",
       "2  A true WW2 story: the British Navy must find a...       97   \n",
       "3  With a flu epidemic running rife, three new bu...       86   \n",
       "4  A man saves a woman who had been kidnapped by ...       73   \n",
       "\n",
       "                             genres  \\\n",
       "0              Action|Drama|Western   \n",
       "1                            Horror   \n",
       "2  Action|Drama|Foreign|History|War   \n",
       "3                            Comedy   \n",
       "4                    Action|Western   \n",
       "\n",
       "                                production_companies release_date  vote_count  \\\n",
       "0                                  James Productions   1960-01-01          17   \n",
       "1  Hammer Film Productions|Hotspur Film Productio...   1960-01-01          19   \n",
       "2             Twentieth Century Fox Film Corporation   1960-02-11          12   \n",
       "3                           Peter Rogers Productions   1960-02-22          10   \n",
       "4  Columbia Pictures Corporation|Ranown Pictures ...   1960-03-01          12   \n",
       "\n",
       "   vote_average  release_year  budget_adj  revenue  \n",
       "0           4.9          1960         0.0      0.0  \n",
       "1           6.6          1960         0.0      0.0  \n",
       "2           4.9          1960         0.0      0.0  \n",
       "3           6.0          1960         0.0      0.0  \n",
       "4           6.5          1960         0.0      0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2).  Using a Custom Loss Function\n",
    "\n",
    "To avoid some of the pitfalls of using a loss function that measures squared error, we're going to modify it a little bit.  This is also a useful skill in practice because lots of projects will require something precise that's not available out-of-the-box in a library.\n",
    "\n",
    "`Scitkit-Learn` allows for custom loss functions relatively easily\n",
    "\n",
    "We're going to instead use the **mean squared log error**.  It has the following form:\n",
    "\n",
    "$$ \\frac{\\sum{log_{e}(y - \\bar{y})^2}}{n} $$\n",
    "\n",
    "The easiest way to do this is the following:\n",
    "\n",
    " - take the log of y using `np.log1p` to avoid the hassles of dealing with negative values\n",
    " - fit your model to that, and then calculate the resulting mean squared error\n",
    " \n",
    "So your job is two fold:\n",
    " - log transform the target variable (revenue)\n",
    " - create a function called `mean_squared_log_error` according to the specifications defined here:  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html, under the heading for the `scoring` argument\n",
    " - to test that you did this correctly, run a 10-fold univariate linear regression on the training set using the `popularity` column as `X` and `revenue` as y.  The correct value should be 60.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transformation: \n",
    "#because r Squared value pays more attention to the larger values and ignores smaller values\n",
    "train['revenue'] = np.log1p(train['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function: Mean Squared Log Error\n",
    "#cannot take the log of a negative number \n",
    "# takes 3 arguments\n",
    "#mean squared error = MSE\n",
    "def mean_squared_log_error(model, X, y):\n",
    "    error = model.predict(X) - y\n",
    "    mse = np.mean(error**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['popularity']]\n",
    "y = train['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=lreg, X=X, y=y, scoring=mean_squared_log_error, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.62649998, 71.38243578, 70.84036913, 65.65364884, 64.9774155 ,\n",
       "       59.57212907, 55.58544199, 52.88254619, 51.54740026, 52.23485259])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.73027393164422"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3).  Distributional Inference of Your Continuous Variables\n",
    "\n",
    "This dataset is far from normal.  Use the `probplot()` method to find the *least* normal variable among your numeric variables, judging by the r-squared value of the resulting line.  \n",
    "\n",
    "Then, see if log-transforming improves its behavior at all.  Use a comparison between your validation scores in a univariate regression between the treated and untreated versions of the variable as your indicator of whether or not this made anything better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hypo distrib vs your data\n",
    "vals, model = probplot(train['vote_count'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating colms with only numerical value, minus the revenue\n",
    "num_cols = num_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove('revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r squared value: ask how this one came about. Nested loop. Can also make the plots individually. \n",
    "rsq = [probplot(train[num_cols[i]])[1][2] for i in range(len(num_cols))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7759321140913117,\n",
       " 0.7484030236487506,\n",
       " 0.7979080090154935,\n",
       " 0.5965756297041135,\n",
       " 0.9934328572787294,\n",
       " 0.9368789697671646,\n",
       " 0.7803944624567354]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = rsq.index(min(rsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vote_count'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols[min]\n",
    "#this is the least normal variable in the data set 0.5965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_score1 = cross_val_score(estimator=lreg, X=train[['vote_count']], y=y, scoring=mean_squared_log_error, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.94013966051395"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_score1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_score2 = cross_val_score(estimator=lreg, X=np.log1p(train[['vote_count']]), y=y, scoring=mean_squared_log_error, cv=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.93631105103633"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_score2.mean() \n",
    "# you want smaller number for the error, r2 want the higher number (close to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4).  Encoding the `Director` Column\n",
    "\n",
    "The `Director` column is a good example of some of the challenges of dealing with categorical data.  If George Lucas or Steven Spielberg direct a film, there's a good chance that has a non-random impact on a film's bottom line.  However, there are a lot of unique values, most of which are probably non-impactful.  \n",
    "\n",
    "Creating a column for everyone is probably not a good idea, but there's also no clear 'order' you could assign them just by looking at their labels.  \n",
    "\n",
    "In this step you're going to try two different techniques to see which one works better on your dataset.\n",
    "\n",
    "**Technique 1:**  Only include directors that have a value count of at least 10 *in your training set*, and set everything else to other.  \n",
    "\n",
    "So:\n",
    "\n",
    " - transform the column accordingly (you can make a new column if that's easier)\n",
    " - transform the same column in your test set so that if a director's name *doesn't* appear in your new training column it gets set to `Other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Woody Allen                                   42\n",
       "Clint Eastwood                                32\n",
       "Steven Spielberg                              28\n",
       "Martin Scorsese                               27\n",
       "Joel Schumacher                               21\n",
       "Brian De Palma                                20\n",
       "Ridley Scott                                  20\n",
       "Ron Howard                                    20\n",
       "Steven Soderbergh                             20\n",
       "Wes Craven                                    19\n",
       "Tim Burton                                    18\n",
       "Mike Nichols                                  18\n",
       "John Carpenter                                18\n",
       "Barry Levinson                                17\n",
       "Norman Jewison                                17\n",
       "Sidney Lumet                                  17\n",
       "David Cronenberg                              17\n",
       "Francis Ford Coppola                          17\n",
       "Oliver Stone                                  17\n",
       "Robert Zemeckis                               16\n",
       "Richard Donner                                16\n",
       "Blake Edwards                                 16\n",
       "Peter Hyams                                   16\n",
       "Rob Reiner                                    16\n",
       "Roman Polanski                                16\n",
       "John Landis                                   16\n",
       "Stephen Herek                                 16\n",
       "Tony Scott                                    16\n",
       "Walter Hill                                   16\n",
       "David Lynch                                   15\n",
       "                                              ..\n",
       "Casey La Scala                                 1\n",
       "Mark McQueen                                   1\n",
       "Ryan Polito                                    1\n",
       "Paul Hyett                                     1\n",
       "Mark Palansky                                  1\n",
       "David Auburn                                   1\n",
       "Inon Shampanier                                1\n",
       "Jang Hoon                                      1\n",
       "Nelson Shin                                    1\n",
       "Larry Ferguson                                 1\n",
       "Christopher Kenneally                          1\n",
       "Bobby Gene Leonard                             1\n",
       "Anthony Hemingway                              1\n",
       "Nick Morris|Laurence Connor                    1\n",
       "Banksy                                         1\n",
       "GÃ©rald Hustache-Mathieu                       1\n",
       "Arnaud Desplechin                              1\n",
       "Jacques Audiard                                1\n",
       "Paul Provenza                                  1\n",
       "Stevan Mena                                    1\n",
       "Mike Gabriel|Eric Goldberg                     1\n",
       "Matteo Garrone                                 1\n",
       "Jeffrey Bloom                                  1\n",
       "Dianne Jackson|Jimmy T. Murakami               1\n",
       "Jonah Markowitz                                1\n",
       "Gregory Mackenzie                              1\n",
       "Jack Starrett                                  1\n",
       "Chris Holt|Stephen Cooter|Michael Lachmann     1\n",
       "Adrian Maben                                   1\n",
       "Dexter Fletcher                                1\n",
       "Name: director, Length: 4035, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform director colms\n",
    "train.director.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_director= train.groupby('director')['director'].transform('count')\n",
    "#Makes a new colm with the director in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12.0\n",
       "1     8.0\n",
       "2     7.0\n",
       "3    12.0\n",
       "4     1.0\n",
       "Name: director, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cts_director.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['top_director'] = np.where(cts_director > 10, train['director'], 'Other')\n",
    "#(where director is greater than 10 it is equal to itself otherwise the value is other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because both the train and the test set have to match.\n",
    "test['top_director'] = np.where(test['director'].isin(train['top_director]), test['director'], 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2:** Use target encoding to transform the column instead, and use the results from your training set to transform your test set.  There are a lot of directors in your test set that are not in your training set, and this will result in missing values.  Fill these in with the column average.\n",
    "\n",
    "**Bonus:** The method we're using here is a little blunt because our average value doesn't account for how often a particular value occurs.  A more nuanced approach to is to take some sort of weighted share between the overall column average and average of your particular unique value.  A good article on this is here:  https://maxhalford.github.io/blog/target-encoding-done-the-right-way/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the groupby methods for the average value of each director's gross revenue\n",
    "#map the values to what we already have in the training and test set. \n",
    "#Dont forget to fill in the missing value on the test set.\n",
    "    #Downside is that does not count how frequently a value occurs.\n",
    "    #better way would be to take the wieghted share. see the link above. \n",
    "#Then concat the data so the train and test colms match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold univariate regression on both to see which one gives you a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint for the colm th you will one hot encode, you can simply pass pd.get_dummies(train['director1']) in the X colm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5).  Standardize Your Data using the `StandardScaler` module\n",
    "\n",
    " - make sure to `fit` it on the training set and `transform` it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A way to transform or standardize\n",
    "# import the module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# we never actually saved this before\n",
    "train['vote_count'] = np.log1p(train['vote_count'])\n",
    "test['vote_count'] = np.log1p(test['vote_count'])\n",
    "\n",
    "# and grab all numberic columns -- these are the ones that won in this case\n",
    "num_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
    "# except for this one\n",
    "num_cols.remove('revenue')\n",
    "\n",
    "# define X\n",
    "X = train[num_cols]\n",
    "# reduce the test set down to the same number of columns\n",
    "test = test[num_cols]\n",
    "\n",
    "# call fit and transform on the training set\n",
    "X = sc.fit_transform(X)\n",
    "# and then transform the test set as well\n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6).  To get an estimate of your models performance, use 10-fold cross validation on your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typically when you want to fit it on all of your training data before submiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7).  Now, before making your final predictions for your test sit, fit the model on all of your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typically when you want to fit it on all of your training data before submiting on your test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8).  Make a prediction on your test set, and save the results as a dataframe, using two columns:\n",
    "\n",
    " - **id**:  the id of your test set rows\n",
    " - **prediction**: your corresponding predictions\n",
    " \n",
    "Submit this to a csv file, using the option `index=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization is penalizing your wieghts of your coeff. \n",
    "#np.expm1 is the inverse log so that you can compare to the original value. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
